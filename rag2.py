import ollama
import os
import json
import numpy as np
from numpy.linalg import norm

# Ouvre un fichier json et renvoie les informations
def parse_file(fichier):
    documents = []
    with open("data/"+fichier, "r", encoding="utf-8") as f:
        documents = json.load(f)
    return documents

# Formate les données pour les passer à l'algo
def formatte_documents(documents):
    doc_format = []
    for doc in documents:
        titre = " | ".join(doc.get("title_s", []))
        abstract = " ".join(doc.get("abstract_s", []))
        mots_cles = ", ".join(doc.get("keywords_s", []) + doc.get("extracted_keywords", []))
        auteurs = ", ".join(doc.get("authors_s", []))
        date = " | ".join(doc.get("date_s", []))

        texte = f"Titre: {titre}\nAbstract: {abstract}\Mots-clés: {mots_cles}\Auteurs: {auteurs}\Date: {date}"
        doc_format.append(texte)
    return doc_format

def sauvegarde_embeddings(fichier, embeddings):
    # Crée un dossier pour les embeddings s'il n'existe pas
    if not os.path.exists("embeddings"):
        os.makedirs("embeddings")
    # Sauvegarde les embeddings dans un fichier json
    with open(f"embeddings/{fichier}.json", "w", encoding="utf-8") as f:
        json.dump(embeddings, f)


def chargement_embeddings(fichier):
    # Vérifie si les embeddings sont déjà sauvegardés
    if not os.path.exists(f"embeddings/{fichier}.json"):
        return False
    # Charge les embeddings
    with open(f"embeddings/{fichier}.json", "r", encoding="utf-8") as f:
        return json.load(f)


def get_embeddings(fichier, nom_modele, chunks):
    # Vérifie si les embeddings sont déjà sauvegardés
    if (embeddings := chargement_embeddings(fichier)) is not False:
        return embeddings
    # Fonction pour sauvegarder les embeddings
    embeddings = [
        ollama.embeddings(model=nom_modele, prompt=chunk)["embedding"]
        for chunk in chunks
    ]
    # Sauvegarde les embeddings
    sauvegarde_embeddings(fichier, embeddings)
    return embeddings

# Fonction pour trouver les embeddings les plus similaires
def trouve_similaire(needle, haystack):
    needle_norm = norm(needle)
    similarity_scores = [
        np.dot(needle, item) / (needle_norm * norm(item)) for item in haystack
    ]
    return sorted(zip(similarity_scores, range(len(haystack))), reverse=True)

SYSTEM_PROMPT = """You are a helpful reading assistant who answers questions 
    based on snippets of text provided in context. Answer only using the context provided, 
    being as concise as possible. If you're unsure, just say that you don't know.
    Context:
"""

# Ouverture du fichier json et récupération des données
filename = "documentsExtractedKeywords.json"
documents = parse_file(filename)
documents_formattes = formatte_documents(documents)

embeddings = get_embeddings(filename, "nomic-embed-text", documents_formattes)

prompt = input("what do you want to know? -> ")
# strongly recommended that all embeddings are generated by the same model (don't mix and match)
prompt_embedding = ollama.embeddings(model="nomic-embed-text", prompt=prompt)["embedding"]
# find most similar to each other
most_similar_chunks = trouve_similaire(prompt_embedding, embeddings)[:5]

response = ollama.chat(
    model="mistral",
    messages=[
        {
            "role": "system",
            "content": SYSTEM_PROMPT
            + "\n".join(documents_formattes[item[1]] for item in most_similar_chunks),
        },
        {"role": "user", "content": prompt},
    ],
)
print("\n\n")
print(response["message"]["content"])